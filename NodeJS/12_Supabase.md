# Supabase: A Complete Developer Guide

## What is Supabase and Why It Exists

Backend-as-a-Service (BaaS) platforms remove the need to build and manage server infrastructure yourself. Rather than provisioning servers, writing API layers, and managing databases, you integrate a hosted service that handles all of that on your behalf.

Supabase is an open-source BaaS platform built on top of PostgreSQL, a battle-tested, full-featured relational database engine. Where Firebase (Google's BaaS) uses a proprietary NoSQL database, Supabase gives you a real PostgreSQL database — the same kind of database used by major production applications across the industry. This means you get the full power of SQL: joins between tables, foreign key constraints, complex queries, transactions, stored procedures, and every other feature SQL developers rely on.

The tagline "the open-source Firebase alternative" is accurate in scope. Supabase provides authentication, a database, file storage, real-time subscriptions, edge functions, and auto-generated APIs — everything you need to build a full application without managing backend infrastructure.

This guide covers every major Supabase service, how each one works conceptually and technically, and how to integrate them into a real application from scratch.

---

## Core Concepts Before You Begin

### PostgreSQL as the Foundation

Every Supabase project is, at its core, a PostgreSQL database. Everything else — the auto-generated REST API, the real-time subscriptions, Row Level Security — is built as a layer on top of that database. If you understand SQL, you already understand Supabase's data model. If you are new to SQL, this guide will explain the relevant concepts as they arise.

A **table** in SQL is what a **collection** is in Firestore or MongoDB: a structured container for records of the same type. Each record (called a **row**) has a fixed set of fields (called **columns**), and every column has a declared data type: `text`, `integer`, `boolean`, `timestamp`, `uuid`, and so on.

Because data is relational, tables can reference each other. A `comments` table can have a `post_id` column that references the `id` column of a `posts` table. This relationship is enforced by the database itself through **foreign key constraints** — the database will refuse to insert a comment that references a post that does not exist.

### The Supabase Client

Supabase provides a JavaScript client library (`@supabase/supabase-js`) and clients for other languages (Python, Swift, Kotlin, Flutter). This library communicates with your project's auto-generated REST API (powered by PostgREST), its Realtime server, Storage server, and Auth server. You install it in your application and use it to interact with all of Supabase's services through a unified interface.

### Row Level Security (RLS)

Row Level Security is PostgreSQL's native mechanism for restricting which rows a user can access in a query. Rather than writing authorization logic in your application code, you write **policies** directly in the database that define access rules. When a request comes in through the Supabase client, the database evaluates the relevant policies before returning or modifying any data.

RLS is the most important security concept in Supabase. A table without RLS policies is either fully open to all authenticated users or fully open to the public — neither of which is typically what you want. This guide covers RLS in depth.

### Auto-Generated REST API

Every table you create in Supabase automatically gets a REST API endpoint generated by PostgREST. You can query, insert, update, and delete rows through this API — and the Supabase client library wraps these API calls in a clean JavaScript interface. You never write the raw HTTP requests; the client handles that. But understanding that this API exists helps you understand why the client's query syntax looks the way it does.

---

## Setting Up Supabase

### Step 1: Create a Supabase Project

Go to [supabase.com](https://supabase.com) and sign up or log in. Click **New project**. You will be prompted to:

Name your project, set a **database password** (this is the password for direct PostgreSQL access — store it securely), and select a region (choose the region closest to your users). The project takes roughly two minutes to provision.

Once created, you land on the project dashboard. In the left sidebar, you can navigate to the Table Editor, Authentication, Storage, Edge Functions, and the SQL Editor.

### Step 2: Get Your Project Credentials

Go to **Project Settings** (gear icon) and then **API**. You will find two key values:

The **Project URL** is the base URL for all API calls to your project. It looks like `https://your-project-ref.supabase.co`.

The **anon key** is a public API key. It is safe to include in your client-side code because access control is enforced by Row Level Security. The `anon` key identifies your project but does not bypass RLS policies.

The **service_role key** is a server-side secret. It bypasses all RLS policies and has full database access. Never expose it in client-side code. Use it only in server-side environments like Edge Functions or your own backend.

### Step 3: Install the Supabase Client

```bash
npm install @supabase/supabase-js
```

### Step 4: Initialize the Supabase Client

Create a `supabase.js` file in your source directory:

```javascript
// src/supabase.js
import { createClient } from '@supabase/supabase-js';

const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY;

export const supabase = createClient(supabaseUrl, supabaseAnonKey);
```

Store the actual values in a `.env` file at the root of your project. The `VITE_` prefix is for Vite-based projects. Use `REACT_APP_` for Create React App, or `NEXT_PUBLIC_` for Next.js:

```
VITE_SUPABASE_URL=https://your-project-ref.supabase.co
VITE_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

Add `.env` to your `.gitignore` to prevent committing credentials to version control.

### Step 5: Install the Supabase CLI

The CLI is used for managing database migrations, generating TypeScript types, running a local development stack, and deploying Edge Functions.

```bash
npm install -g supabase
```

Log in to your Supabase account:

```bash
supabase login
```

This opens a browser window. After authentication, initialize Supabase in your project directory:

```bash
supabase init
```

This creates a `supabase/` directory in your project with the following structure:

```
supabase/
  config.toml       ← Configuration for the local development stack
  migrations/       ← SQL migration files, one per schema change
  seed.sql          ← SQL to populate the local database with test data
```

Link the CLI to your remote Supabase project:

```bash
supabase link --project-ref your-project-ref
```

Your project reference is the string in your project URL: `https://your-project-ref.supabase.co`.

---

## Database: Working with PostgreSQL

### Creating Tables

You can create tables through the Table Editor in the Supabase dashboard (a visual interface) or through the SQL Editor. For any serious project, you should manage your schema through **migrations** — SQL files that describe each change to your database schema. This creates a version history of your schema and allows you to reproduce your database structure in any environment.

Here is a complete schema for a blog application, written as a SQL migration:

```sql
-- Create the posts table
create table public.posts (
  -- uuid is a universally unique identifier, better than sequential integers for security
  id uuid default gen_random_uuid() primary key,

  -- references auth.users links this to Supabase's built-in users table
  -- on delete cascade means if a user is deleted, all their posts are deleted too
  author_id uuid references auth.users(id) on delete cascade not null,

  title text not null,
  body text not null,
  published boolean default false not null,

  -- timestamptz is timestamp with timezone — always use this over plain timestamp
  created_at timestamptz default now() not null,
  updated_at timestamptz default now() not null
);

-- Create the comments table
create table public.comments (
  id uuid default gen_random_uuid() primary key,
  post_id uuid references public.posts(id) on delete cascade not null,
  author_id uuid references auth.users(id) on delete cascade not null,
  body text not null,
  created_at timestamptz default now() not null
);

-- Create the user_profiles table to store public user information
-- Supabase's auth.users table is private — store anything you want users to see here
create table public.user_profiles (
  -- The id here matches the id in auth.users, creating a one-to-one relationship
  id uuid references auth.users(id) on delete cascade primary key,
  username text unique not null,
  display_name text,
  avatar_url text,
  bio text,
  created_at timestamptz default now() not null
);
```

Create a migration file with:

```bash
supabase migration new create_blog_tables
```

This creates a timestamped file in `supabase/migrations/`. Paste your SQL into it, then apply the migration to your local database:

```bash
supabase db reset
```

To apply migrations to your remote (production) database:

```bash
supabase db push
```

### Reading Data

The Supabase client provides a query builder that maps closely to SQL. Understanding the SQL equivalent of each operation helps demystify how it works.

```javascript
import { supabase } from './supabase';

// --- Fetch all published posts ---
// SQL equivalent: SELECT * FROM posts WHERE published = true ORDER BY created_at DESC
const { data: posts, error } = await supabase
  .from('posts')
  .select('*')
  .eq('published', true)
  .order('created_at', { ascending: false });

if (error) {
  console.error('Query failed:', error.message);
} else {
  console.log('Posts:', posts);
}

// --- Fetch a single post by ID ---
// SQL equivalent: SELECT * FROM posts WHERE id = 'abc123' LIMIT 1
const { data: post, error } = await supabase
  .from('posts')
  .select('*')
  .eq('id', postId)
  .single(); // .single() returns an object instead of an array, and errors if 0 or 2+ rows match

// --- Select specific columns ---
const { data, error } = await supabase
  .from('posts')
  .select('id, title, created_at')
  .eq('published', true);

// --- Pagination ---
const PAGE_SIZE = 10;
const page = 0; // Zero-indexed

const { data, error } = await supabase
  .from('posts')
  .select('*')
  .eq('published', true)
  .order('created_at', { ascending: false })
  .range(page * PAGE_SIZE, (page + 1) * PAGE_SIZE - 1);
  // SQL equivalent: LIMIT 10 OFFSET 0
```

### Joining Tables

One of Supabase's most powerful features compared to document databases is the ability to join related tables in a single query. Because Supabase knows about your foreign key relationships, the client library can infer how to join tables automatically using a syntax called **embedded resources**.

```javascript
// Fetch posts with their author's profile
// This performs a SQL JOIN between posts and user_profiles
const { data: posts, error } = await supabase
  .from('posts')
  .select(`
    id,
    title,
    body,
    created_at,
    user_profiles (
      username,
      display_name,
      avatar_url
    )
  `)
  .eq('published', true)
  .order('created_at', { ascending: false });

// Each post object in the result will look like:
// {
//   id: 'abc123',
//   title: 'Getting Started with Supabase',
//   body: '...',
//   created_at: '2024-01-15T10:00:00Z',
//   user_profiles: {
//     username: 'alexsmith',
//     display_name: 'Alex Smith',
//     avatar_url: 'https://...'
//   }
// }

// Fetch a post with its author AND all its comments with comment authors
const { data: post, error } = await supabase
  .from('posts')
  .select(`
    id,
    title,
    body,
    user_profiles ( username, avatar_url ),
    comments (
      id,
      body,
      created_at,
      user_profiles ( username, avatar_url )
    )
  `)
  .eq('id', postId)
  .single();
```

A query like this would require multiple round-trips or a complex API endpoint if you were building a traditional backend. In Supabase, it is a single query that the database resolves efficiently.

### Writing Data

```javascript
// --- Insert a new row ---
const { data, error } = await supabase
  .from('posts')
  .insert({
    title: 'My First Post',
    body: 'Supabase makes backend development much simpler.',
    author_id: currentUserId,
    published: false
  })
  .select() // Returns the inserted row
  .single();

if (error) {
  console.error('Insert failed:', error.message);
} else {
  console.log('Created post:', data);
}

// --- Update a row ---
const { data, error } = await supabase
  .from('posts')
  .update({
    published: true,
    updated_at: new Date().toISOString()
  })
  .eq('id', postId)
  .select()
  .single();

// --- Upsert (insert if not exists, update if exists) ---
// Useful for things like user profiles: create it on first sign-in, update it on subsequent ones
const { data, error } = await supabase
  .from('user_profiles')
  .upsert({
    id: userId,
    username: 'alexsmith',
    display_name: 'Alex Smith'
  }, {
    onConflict: 'id' // Which column to check for conflicts
  })
  .select()
  .single();

// --- Delete a row ---
const { error } = await supabase
  .from('posts')
  .delete()
  .eq('id', postId);
```

### Filtering Data

The Supabase client supports all the comparison operators you would use in SQL:

```javascript
// .eq('column', value)     — column = value
// .neq('column', value)    — column != value
// .gt('column', value)     — column > value
// .gte('column', value)    — column >= value
// .lt('column', value)     — column < value
// .lte('column', value)    — column <= value
// .like('column', pattern) — column LIKE 'pattern' (case-sensitive)
// .ilike('column', pattern)— column ILIKE 'pattern' (case-insensitive)
// .in('column', [values])  — column IN (value1, value2, ...)
// .is('column', null)      — column IS NULL
// .not('column', 'is', null) — column IS NOT NULL

// Case-insensitive search for posts containing "firebase" in the title
const { data, error } = await supabase
  .from('posts')
  .select('id, title')
  .ilike('title', '%firebase%');

// Get posts created in the last 7 days
const sevenDaysAgo = new Date();
sevenDaysAgo.setDate(sevenDaysAgo.getDate() - 7);

const { data, error } = await supabase
  .from('posts')
  .select('id, title, created_at')
  .gte('created_at', sevenDaysAgo.toISOString())
  .order('created_at', { ascending: false });
```

### Using the SQL Editor for Complex Queries

The Supabase dashboard includes a full SQL Editor where you can run arbitrary SQL queries against your database. For operations that are difficult to express through the query builder — complex aggregations, window functions, multi-table updates — write raw SQL:

```sql
-- Get post count and latest post date per author
SELECT
  up.username,
  up.display_name,
  COUNT(p.id) as post_count,
  MAX(p.created_at) as last_posted_at
FROM user_profiles up
LEFT JOIN posts p ON p.author_id = up.id AND p.published = true
GROUP BY up.id, up.username, up.display_name
ORDER BY post_count DESC;
```

You can also run raw SQL from the client using `supabase.rpc()` (covered in the Row Level Security section) or by calling database functions.

---

## Row Level Security (RLS)

### Why RLS Exists

When you query Supabase from a browser, the request goes from the user's device directly to the database API. Unlike a traditional backend where you control a server that validates every request, in Supabase the client calls the API directly. This is only safe if the database itself enforces access control.

Row Level Security is PostgreSQL's built-in mechanism for this. When RLS is enabled on a table, every query against that table is filtered through a set of policies that you define. A user trying to read another user's private data simply gets back an empty result set — not a permission error, and not data they should not see.

### Enabling RLS

RLS must be explicitly enabled on each table. By default, tables have no RLS, meaning they are either fully accessible (if you allow it in your API settings) or follow whatever your API's default policy is. Enable it in SQL:

```sql
alter table public.posts enable row level security;
alter table public.comments enable row level security;
alter table public.user_profiles enable row level security;
```

Once enabled, **a table with no policies allows no access at all**. You must explicitly grant access through policies.

### Writing RLS Policies

A policy consists of three parts: a name, the operation it applies to (`SELECT`, `INSERT`, `UPDATE`, `DELETE`, or `ALL`), and a boolean expression that must be true for the operation to proceed.

Two special objects are available inside policy expressions:

`auth.uid()` returns the UUID of the currently authenticated user, or `null` if the user is not authenticated.

`auth.role()` returns `'anon'` for unauthenticated users and `'authenticated'` for signed-in users.

Here are complete RLS policies for the blog application:

```sql
-- ============================================================
-- POSTS TABLE POLICIES
-- ============================================================

-- Anyone can read published posts (including anonymous visitors)
create policy "Anyone can read published posts"
  on public.posts
  for select
  using (published = true);

-- Authors can read their own unpublished drafts
create policy "Authors can read own drafts"
  on public.posts
  for select
  using (auth.uid() = author_id);

-- Authenticated users can create posts
-- WITH CHECK validates the data being inserted/updated
-- USING filters which rows are visible for SELECT/UPDATE/DELETE
create policy "Authenticated users can create posts"
  on public.posts
  for insert
  with check (
    auth.uid() is not null
    and auth.uid() = author_id
  );

-- Authors can update their own posts
create policy "Authors can update own posts"
  on public.posts
  for update
  using (auth.uid() = author_id)
  with check (auth.uid() = author_id);

-- Authors can delete their own posts
create policy "Authors can delete own posts"
  on public.posts
  for delete
  using (auth.uid() = author_id);

-- ============================================================
-- COMMENTS TABLE POLICIES
-- ============================================================

-- Anyone can read comments on published posts
create policy "Anyone can read comments on published posts"
  on public.comments
  for select
  using (
    exists (
      select 1 from public.posts
      where posts.id = comments.post_id
      and posts.published = true
    )
  );

-- Authenticated users can create comments on published posts
create policy "Authenticated users can comment on published posts"
  on public.comments
  for insert
  with check (
    auth.uid() is not null
    and auth.uid() = author_id
    and exists (
      select 1 from public.posts
      where posts.id = comments.post_id
      and posts.published = true
    )
  );

-- Comment authors can delete their own comments
create policy "Authors can delete own comments"
  on public.comments
  for delete
  using (auth.uid() = author_id);

-- ============================================================
-- USER PROFILES TABLE POLICIES
-- ============================================================

-- All authenticated users can read all profiles
create policy "Authenticated users can read profiles"
  on public.user_profiles
  for select
  using (auth.role() = 'authenticated');

-- Users can only insert their own profile
create policy "Users can insert own profile"
  on public.user_profiles
  for insert
  with check (auth.uid() = id);

-- Users can only update their own profile
create policy "Users can update own profile"
  on public.user_profiles
  for update
  using (auth.uid() = id)
  with check (auth.uid() = id);
```

Apply these policies through the SQL Editor in the Supabase dashboard, or add them to your migration files.

### Bypassing RLS with the Service Role

Server-side code (Edge Functions, your own backend) that should have full database access uses the `service_role` key when creating the client:

```javascript
// Server-side only — never use service_role key in the browser
import { createClient } from '@supabase/supabase-js';

const supabaseAdmin = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
);

// This query bypasses all RLS policies
const { data, error } = await supabaseAdmin
  .from('posts')
  .select('*');
```

---

## Supabase Auth

### The Problem It Solves

Authentication requires securely storing passwords (using proper hashing algorithms), managing session tokens, handling OAuth flows with third-party providers, and protecting against common attacks. Supabase Auth handles all of this, built on top of GoTrue — a battle-tested open-source authentication server.

When a user signs in, Supabase Auth creates a session and stores a JWT (JSON Web Token) in the browser. Every subsequent request your client makes includes this token. When the request reaches your database, Supabase extracts the user's ID from the token and makes it available as `auth.uid()` — which is exactly what your RLS policies use.

### Enabling Auth Providers

In the Supabase dashboard, go to **Authentication** and then **Providers**. Email/password authentication is enabled by default. For OAuth providers like Google, GitHub, or Discord, toggle them on and provide your OAuth application credentials from each provider.

For GitHub, for example, create an OAuth application at `github.com/settings/developers`, set the callback URL to `https://your-project-ref.supabase.co/auth/v1/callback`, then paste the client ID and secret into Supabase.

### Email and Password Authentication

```javascript
import { supabase } from './supabase';

// Sign up a new user
async function signUp(email, password) {
  const { data, error } = await supabase.auth.signUp({
    email,
    password,
    options: {
      // Optional: additional data to store on the user object
      data: {
        display_name: 'New User'
      }
    }
  });

  if (error) {
    console.error('Sign up failed:', error.message);
    throw error;
  }

  // data.user is the new user object
  // data.session is the active session (null if email confirmation is required)
  console.log('User created:', data.user.id);
  return data;
}

// Sign in an existing user
async function signIn(email, password) {
  const { data, error } = await supabase.auth.signInWithPassword({
    email,
    password
  });

  if (error) {
    // error.message will describe the issue: 'Invalid login credentials', etc.
    console.error('Sign in failed:', error.message);
    throw error;
  }

  return data;
}

// Sign out
async function signOut() {
  const { error } = await supabase.auth.signOut();
  if (error) throw error;
}

// Request a password reset email
async function resetPassword(email) {
  const { error } = await supabase.auth.resetPasswordForEmail(email, {
    redirectTo: 'https://your-app.com/update-password'
  });
  if (error) throw error;
}

// Update the password (called after the user clicks the reset link and lands on /update-password)
async function updatePassword(newPassword) {
  const { error } = await supabase.auth.updateUser({
    password: newPassword
  });
  if (error) throw error;
}
```

### OAuth Sign-In

```javascript
// Redirect to the OAuth provider and back to your app
async function signInWithGitHub() {
  const { data, error } = await supabase.auth.signInWithOAuth({
    provider: 'github',
    options: {
      redirectTo: window.location.origin + '/auth/callback'
    }
  });

  if (error) throw error;
  // data.url is the URL the user is redirected to (handled automatically)
}

async function signInWithGoogle() {
  const { error } = await supabase.auth.signInWithOAuth({
    provider: 'google'
  });
  if (error) throw error;
}
```

### Listening for Auth State Changes

```javascript
// onAuthStateChange fires immediately with the current state,
// and then fires again on every auth event: SIGNED_IN, SIGNED_OUT, TOKEN_REFRESHED, etc.
const { data: { subscription } } = supabase.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_IN') {
    console.log('User signed in:', session.user.id);
    // Update your app state
  } else if (event === 'SIGNED_OUT') {
    console.log('User signed out');
    // Clear your app state
  } else if (event === 'TOKEN_REFRESHED') {
    console.log('Session token refreshed');
  }
});

// Stop listening when appropriate (e.g., component unmount)
subscription.unsubscribe();
```

### Getting the Current Session and User

```javascript
// Get the current session synchronously from the local cache
// This is fast and does not make a network request
const { data: { session } } = await supabase.auth.getSession();

if (session) {
  const user = session.user;
  console.log('User ID:', user.id);
  console.log('Email:', user.email);
  console.log('Provider:', user.app_metadata.provider); // 'email', 'github', etc.
} else {
  console.log('No active session');
}
```

### Automatically Creating User Profiles

A common pattern is to create a row in `user_profiles` every time a new user signs up. In Supabase, you do this with a PostgreSQL trigger and function — a piece of SQL that runs automatically when a new row is inserted into the `auth.users` table.

```sql
-- Create a function that inserts a profile when a user is created
create or replace function public.handle_new_user()
returns trigger
language plpgsql
security definer set search_path = public
as $$
begin
  insert into public.user_profiles (id, display_name, avatar_url)
  values (
    new.id,
    new.raw_user_meta_data ->> 'display_name',
    new.raw_user_meta_data ->> 'avatar_url'
  );
  return new;
end;
$$;

-- Create a trigger that calls the function after every insert on auth.users
create or replace trigger on_auth_user_created
  after insert on auth.users
  for each row execute procedure public.handle_new_user();
```

Add this to a migration file and apply it. Now every new user automatically has a corresponding `user_profiles` row created for them.

---

## Real-Time Subscriptions

### How It Works

Supabase real-time is built on PostgreSQL's Logical Replication feature. When a row is inserted, updated, or deleted in a table you are subscribed to, PostgreSQL's internal change log captures the event and Supabase broadcasts it to all connected clients over a WebSocket connection.

This enables features like live chat, real-time collaborative editing, live dashboards, and notification systems — all without polling.

### Subscribing to Database Changes

```javascript
import { supabase } from './supabase';

// Subscribe to all changes in the 'comments' collection for a specific post
function subscribeToComments(postId, callback) {
  const channel = supabase
    .channel('post-comments-' + postId) // A unique name for this subscription
    .on(
      'postgres_changes',
      {
        event: '*',      // Listen for INSERT, UPDATE, and DELETE
        schema: 'public',
        table: 'comments',
        filter: `post_id=eq.${postId}` // Only changes matching this filter
      },
      (payload) => {
        // payload.eventType is 'INSERT', 'UPDATE', or 'DELETE'
        // payload.new is the new row data (for INSERT and UPDATE)
        // payload.old is the old row data (for UPDATE and DELETE)
        console.log('Change received:', payload.eventType);
        callback(payload);
      }
    )
    .subscribe();

  // Return a cleanup function
  return () => supabase.removeChannel(channel);
}

// Usage in a component
const cleanup = subscribeToComments(postId, (payload) => {
  if (payload.eventType === 'INSERT') {
    setComments(prev => [...prev, payload.new]);
  } else if (payload.eventType === 'DELETE') {
    setComments(prev => prev.filter(c => c.id !== payload.old.id));
  } else if (payload.eventType === 'UPDATE') {
    setComments(prev => prev.map(c => c.id === payload.new.id ? payload.new : c));
  }
});

// When the component unmounts
cleanup();
```

### Subscribing to Specific Events

```javascript
// Subscribe only to new inserts
const channel = supabase
  .channel('new-posts')
  .on(
    'postgres_changes',
    {
      event: 'INSERT',
      schema: 'public',
      table: 'posts',
      filter: 'published=eq.true'
    },
    (payload) => {
      console.log('New post published:', payload.new);
    }
  )
  .subscribe();
```

### Enabling Real-Time for Your Tables

By default, Supabase does not replicate all tables to avoid unnecessary overhead. You must explicitly enable real-time for each table you want to subscribe to. Do this in the Supabase dashboard under **Database > Replication**, or via SQL:

```sql
-- Enable real-time for the comments table
alter publication supabase_realtime add table public.comments;

-- Enable real-time for the posts table
alter publication supabase_realtime add table public.posts;
```

---

## Supabase Storage

### The Problem It Solves

Supabase Storage provides managed file storage backed by S3-compatible object storage. Users can upload files (images, videos, PDFs, etc.) directly from the browser, secured by policies that integrate with your database's Row Level Security.

### Creating Storage Buckets

A **bucket** is a named container for files, similar to a top-level folder. You create buckets in the Supabase dashboard under **Storage**, or via SQL:

```sql
insert into storage.buckets (id, name, public)
values ('avatars', 'avatars', true);
-- public: true means files are readable by anyone without authentication
-- Set to false for private files

insert into storage.buckets (id, name, public)
values ('documents', 'documents', false);
```

### Storage Policies

Like tables, storage buckets are secured with policies. Storage policies use the same syntax as database policies:

```sql
-- Allow authenticated users to upload their own avatar
create policy "Users can upload own avatar"
  on storage.objects
  for insert
  with check (
    bucket_id = 'avatars'
    and auth.uid() is not null
    and (storage.foldername(name))[1] = auth.uid()::text
    -- The file path must start with the user's ID: 'user-id/avatar.jpg'
  );

-- Allow anyone to view avatars (bucket is public, but policy still applies)
create policy "Avatars are publicly viewable"
  on storage.objects
  for select
  using (bucket_id = 'avatars');

-- Allow users to update and delete their own avatars
create policy "Users can manage own avatar"
  on storage.objects
  for update, delete
  using (
    bucket_id = 'avatars'
    and auth.uid()::text = (storage.foldername(name))[1]
  );

-- Private documents: only the owner can read, write, and delete
create policy "Users can manage own documents"
  on storage.objects
  for all
  using (
    bucket_id = 'documents'
    and auth.uid()::text = (storage.foldername(name))[1]
  )
  with check (
    bucket_id = 'documents'
    and auth.uid()::text = (storage.foldername(name))[1]
  );
```

### Uploading Files

```javascript
import { supabase } from './supabase';

async function uploadAvatar(file) {
  const user = (await supabase.auth.getUser()).data.user;
  if (!user) throw new Error('Not authenticated');

  // Construct the file path: user's ID / filename
  // This path must match your storage policy
  const filePath = `${user.id}/avatar.${file.name.split('.').pop()}`;

  const { data, error } = await supabase.storage
    .from('avatars')   // The bucket name
    .upload(filePath, file, {
      cacheControl: '3600',  // Browser cache duration in seconds
      upsert: true           // Replace if a file at this path already exists
    });

  if (error) {
    console.error('Upload failed:', error.message);
    throw error;
  }

  console.log('Uploaded to:', data.path);
  return data.path;
}
```

### Getting a Public URL

For public buckets, generate a permanent public URL:

```javascript
function getAvatarUrl(filePath) {
  const { data } = supabase.storage
    .from('avatars')
    .getPublicUrl(filePath);

  return data.publicUrl;
  // Returns: https://your-project-ref.supabase.co/storage/v1/object/public/avatars/user-id/avatar.jpg
}
```

### Getting a Signed URL for Private Files

For private buckets, generate a temporary signed URL that expires after a set duration:

```javascript
async function getDocumentUrl(filePath) {
  const { data, error } = await supabase.storage
    .from('documents')
    .createSignedUrl(filePath, 3600); // Expires in 3600 seconds (1 hour)

  if (error) throw error;
  return data.signedUrl;
}
```

### Listing and Deleting Files

```javascript
// List files in a path (within a bucket)
async function listUserFiles(userId) {
  const { data: files, error } = await supabase.storage
    .from('documents')
    .list(userId + '/', {
      limit: 100,
      offset: 0,
      sortBy: { column: 'created_at', order: 'desc' }
    });

  if (error) throw error;
  return files;
}

// Delete a file
async function deleteFile(filePath) {
  const { error } = await supabase.storage
    .from('documents')
    .remove([filePath]);

  if (error) throw error;
}
```

---

## Edge Functions

### The Problem They Solve

Some logic must run server-side: calling third-party APIs whose secret keys cannot be exposed to the client, processing webhooks from services like Stripe or GitHub, sending transactional emails, running scheduled jobs, or performing computations that would expose business logic you want to protect.

Supabase Edge Functions are server-side TypeScript (or JavaScript) functions powered by Deno, deployed to Supabase's global edge infrastructure. They run close to your users geographically and scale automatically.

### Creating an Edge Function

```bash
supabase functions new send-welcome-email
```

This creates `supabase/functions/send-welcome-email/index.ts`. Write your function logic here:

```typescript
// supabase/functions/send-welcome-email/index.ts
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

serve(async (req) => {
  try {
    // Parse the request body
    const { userId, email } = await req.json();

    // Create an admin Supabase client to bypass RLS
    const supabaseAdmin = createClient(
      Deno.env.get('SUPABASE_URL')!,
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    );

    // Fetch the user's profile
    const { data: profile } = await supabaseAdmin
      .from('user_profiles')
      .select('display_name')
      .eq('id', userId)
      .single();

    // Send the email using a service like Resend, SendGrid, etc.
    const emailResponse = await fetch('https://api.resend.com/emails', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${Deno.env.get('RESEND_API_KEY')}`
      },
      body: JSON.stringify({
        from: 'noreply@yourapp.com',
        to: email,
        subject: 'Welcome to Our App',
        html: `<h1>Welcome, ${profile?.display_name || 'there'}!</h1>
               <p>We are glad you joined.</p>`
      })
    });

    if (!emailResponse.ok) {
      throw new Error('Email sending failed');
    }

    return new Response(
      JSON.stringify({ success: true }),
      { headers: { 'Content-Type': 'application/json' }, status: 200 }
    );
  } catch (error) {
    return new Response(
      JSON.stringify({ error: error.message }),
      { headers: { 'Content-Type': 'application/json' }, status: 500 }
    );
  }
});
```

### Setting Environment Variables for Functions

Secret keys used in Edge Functions are set as environment variables, separate from your local `.env` file:

```bash
supabase secrets set RESEND_API_KEY=re_xxxxxxxxxxxxxxxx
```

For local development, create a `.env` file inside `supabase/functions/` (not the project root):

```
RESEND_API_KEY=re_xxxxxxxxxxxxxxxx
```

### Testing Functions Locally

```bash
supabase functions serve send-welcome-email --env-file supabase/functions/.env
```

This starts a local server for your function. Test it with curl:

```bash
curl -i --location --request POST 'http://localhost:54321/functions/v1/send-welcome-email' \
  --header 'Authorization: Bearer YOUR_ANON_KEY' \
  --header 'Content-Type: application/json' \
  --data '{"userId": "some-user-id", "email": "test@example.com"}'
```

### Deploying Functions

```bash
supabase functions deploy send-welcome-email
```

### Calling Functions from the Client

```javascript
import { supabase } from './supabase';

async function sendWelcomeEmail(userId, email) {
  const { data, error } = await supabase.functions.invoke('send-welcome-email', {
    body: { userId, email }
  });

  if (error) throw error;
  return data;
}
```

---

## Database Functions and Stored Procedures

PostgreSQL allows you to write functions that live inside the database and can be called directly. This is useful for encapsulating complex business logic, performing operations that require multiple steps atomically, or creating reusable query logic that benefits from being executed close to the data.

```sql
-- A function that increments a post's view count
-- SECURITY DEFINER means it runs with the permissions of the function owner,
-- bypassing RLS — use carefully
create or replace function increment_view_count(post_id uuid)
returns void
language plpgsql
as $$
begin
  update public.posts
  set view_count = view_count + 1
  where id = post_id;
end;
$$;
```

Call it from the client using `rpc()` (Remote Procedure Call):

```javascript
const { error } = await supabase.rpc('increment_view_count', {
  post_id: postId
});
```

A more practical example: a search function using PostgreSQL's full-text search:

```sql
-- Function for full-text search across post titles and bodies
create or replace function search_posts(search_query text)
returns setof public.posts
language sql
as $$
  select *
  from public.posts
  where
    published = true
    and (
      to_tsvector('english', title || ' ' || body) @@ plainto_tsquery('english', search_query)
    )
  order by
    ts_rank(to_tsvector('english', title || ' ' || body), plainto_tsquery('english', search_query)) desc;
$$;
```

```javascript
const { data: results, error } = await supabase
  .rpc('search_posts', { search_query: 'firebase authentication' });
```

---

## Local Development with the Supabase CLI

### Starting the Local Stack

```bash
supabase start
```

This command uses Docker to spin up a complete local Supabase environment on your machine:

```
Started supabase local development setup.

         API URL: http://localhost:54321
     GraphQL URL: http://localhost:54321/graphql/v1
          DB URL: postgresql://postgres:postgres@localhost:54322/postgres
      Studio URL: http://localhost:54323
    Inbucket URL: http://localhost:54324
      JWT secret: super-secret-jwt-token-with-at-least-32-characters-long
        anon key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
service_role key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

Use the local `anon key` and `API URL` in your development environment variables. The Studio URL opens a local version of the Supabase dashboard where you can inspect your local database, manage users, and view storage — all without affecting your production project.

The Inbucket URL is a local email inbox. When your local auth system sends verification emails or password reset emails, they are captured here so you can view them during development without needing a real email service.

### Connecting to the Local Database

Update your `.env.development` to point to the local Supabase instance:

```
VITE_SUPABASE_URL=http://localhost:54321
VITE_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9... (local anon key)
```

### Managing Migrations

Create a new migration for a schema change:

```bash
supabase migration new add_view_count_to_posts
```

Write the SQL in the generated file, then apply it locally:

```bash
supabase db reset
```

`db reset` drops and recreates the local database from scratch, applies all migrations in order, and runs `seed.sql`. This ensures your local state always matches your migration history.

To apply only new migrations without resetting:

```bash
supabase migration up
```

Push all local migrations to your remote production database:

```bash
supabase db push
```

### Generating TypeScript Types

Supabase can generate TypeScript types from your database schema, giving you full type safety when querying your database from TypeScript code:

```bash
supabase gen types typescript --local > src/database.types.ts
```

Use the generated types in your client:

```typescript
import { createClient } from '@supabase/supabase-js';
import type { Database } from './database.types';

export const supabase = createClient<Database>(
  import.meta.env.VITE_SUPABASE_URL,
  import.meta.env.VITE_SUPABASE_ANON_KEY
);

// Now your queries are fully typed
// TypeScript will know the exact shape of every row you read or write
const { data, error } = await supabase
  .from('posts')
  .select('id, title, published');
// data is typed as Array<{ id: string; title: string; published: boolean }> | null
```

---

## Supabase Pricing

Supabase offers a **Free plan** and a **Pro plan** ($25/month per project). The free plan includes:

500 MB of database storage, 1 GB of storage bandwidth, 2 GB of file storage, 50,000 monthly active users for authentication, and 500,000 Edge Function invocations per month. The free plan also includes unlimited API requests, limited only by database performance.

The Pro plan adds significantly higher limits across all services, daily database backups, no project pausing (free projects pause after 1 week of inactivity), and access to support.

For most applications in early development, the free plan is more than sufficient.

---

## Environment Separation: Development vs. Production

A production-ready Supabase setup uses two separate projects: one for development and one for production. This isolates your development activity from real users and real data.

Create a second Supabase project in the dashboard for production. Maintain separate environment files:

```
.env.development    ← Local Supabase instance (localhost) or development project
.env.production     ← Production Supabase project credentials
```

In `.env.development`:

```
VITE_SUPABASE_URL=http://localhost:54321
VITE_SUPABASE_ANON_KEY=local-anon-key
```

In `.env.production`:

```
VITE_SUPABASE_URL=https://your-prod-project-ref.supabase.co
VITE_SUPABASE_ANON_KEY=production-anon-key
```

Your build tooling loads the correct file based on the environment. When you are ready to deploy a schema change to production, the workflow is:

Write and test the migration locally, verify it works with `supabase db reset`, commit it to version control, then deploy to production with `supabase db push`. This gives you a reliable, reproducible schema deployment process.

---

## Connecting Supabase to an External Backend

While the Supabase client can be used directly from the browser, some architectures include a custom Node.js backend in front of Supabase. In this case, your Node.js server uses the `service_role` key to interact with Supabase, and your client only communicates with your own API — never directly with Supabase.

```javascript
// In your Node.js / Express backend
import { createClient } from '@supabase/supabase-js';
import express from 'express';

const supabaseAdmin = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY  // Full access, RLS bypassed
);

const app = express();
app.use(express.json());

app.get('/api/posts', async (req, res) => {
  const { data, error } = await supabaseAdmin
    .from('posts')
    .select('id, title, created_at')
    .eq('published', true)
    .order('created_at', { ascending: false });

  if (error) return res.status(500).json({ error: error.message });
  res.json(data);
});
```

This pattern is useful when you need to add custom business logic, integrate with other internal services, or when your security requirements demand that all database access goes through a controlled API layer.
